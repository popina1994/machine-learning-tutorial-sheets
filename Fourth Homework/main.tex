\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{calc}
\pagestyle{empty}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Djordje Zivanovic}
\title{Second Homework from Machine Learning}

\begin{document}

\centerline{\large \bf Machine learning : Sheet 4}
\centerline{\large Author : Djordje Zivanovic}
\bigskip
%Chapter 1 in Ockendon \& Tayler: Applied PDE's  
%{\it Maths Institute Lecture Notes}. 	
%{\bf Exercises 1, 2, 4, 5}.
\newcommand{\transpose}[1]{#1 ^ \text{T}}
\newcommand{\mat}[1]{\textbf{\text{{#1}}}}
\newcommand{\vect}[1]{\mat{#1}}
\newcommand{\gradient}[1]{\nabla_{\vect{#1}}}
\newcommand{\der}[1]{\frac{\partial}{\partial #1}}
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\logfun}{\sigma(\transpose{\vect{x}}_i \vect{w})}
\newcommand{\softmaxi}{\frac{e^{z_i}}{\sum_{l=1}^Ce^{z_l}}}
\newcommand{\layersep}{2.5cm}
\newcommand{\layersepH}{6cm}
\newcommand{\layersepP}{7cm}
\newcommand{\layersepB}{9cm}
\begin{enumerate}
\item 
\begin{align}
\frac{\partial \ell (\mat{W}, \vect{b}, \vect{x}, y)}{\partial \vect{a}} 
&= -\frac{\partial \log a_y}{\partial \vect{a}} \label{eq:logDef}\\
&= \left[ -\frac{\partial \log a_y}{\partial a_1}, ..., -\frac{\partial \log a_y}{\partial a_y}, ..., -\frac{\partial \log a_y}{\partial a_C}  \right] \label{eq:gradDef}\\
&= \left[0, ..., -\frac{1}{a_y}, ..., 0\right] \label{eq:derLog}
\end{align}
The equation \ref{eq:logDef} is a definition of the objective function for a given point. The equation \ref{eq:gradDef} is a definition of a gradient of a scalar function (derivative of scalar by a vector). The equation \ref{eq:derLog} uses property of derivative of a logarithm function.
\begin{align}
\frac{\partial \ell (\mat{W}, \vect{b}, \vect{x}, y)}{\partial \vect{z}} 
&=\frac{\partial \ell (\mat{W}, \vect{b}, \vect{x}, y)}{\partial \vect{a}}
\cdot \frac{\partial \vect{a}}{\partial \vect{z}}   \label{eq:defDer}
\end{align}
From the equation \ref{eq:defDer} we can see that we need to calculate $\frac{\partial\vect{a}}{\partial\vect{z}}$
\begin{align}
\frac{\partial \vect{a}}{\partial \vect{z}} 
&=\frac{\partial \left[ \frac{ e^{z_{1}}}{\sum_{l=1}^{C}e^{z_{l}}}, ..., \frac{ e^{z_{i}}}{\sum_{l=1}^{C}e^{z_{l}}},..., \frac{ e^{z_{C}}}{\sum_{l=1}^{C}e^{z_{l}}} \right]}{\partial \vect{z}}  \label{eq:softMax}\\
&=\begin{bmatrix}
    \frac{\partial \frac{ e^{z_{1}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_1}     & 
    \frac{\partial \frac{ e^{z_{1}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_2}     &  & \dots & \frac{\partial \frac{ e^{z_{1}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_C} \\
    \frac{\partial \frac{ e^{z_{2}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_1}     & 
    \frac{\partial \frac{ e^{z_{2}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_2}     &  & \dots & \frac{\partial \frac{ e^{z_{2}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_C} \\
    \hdotsfor{5} \\
    \frac{\partial \frac{ e^{z_{C}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_1}     & 
    \frac{\partial \frac{ e^{z_{C}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_2}     &  & \dots & \frac{\partial \frac{ e^{z_{C}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_C} \\
\end{bmatrix} \label{eq:matrix}
\end{align}
The equation \ref{eq:softMax} is softmax classifier equation given in the text. 
The equation \ref{eq:matrix} is a Jacobian of derivatives of vectors. 
Let us denote: $$S_i = \frac{ e^{z_{i}}}{\sum_{l=1}^{C}e^{z_{l}}}$$
Then the equation \ref{eq:matrix} becomes:
\begin{align}
\begin{bmatrix}
    \frac{\partial S_1}{\partial z_1}     & 
    \frac{\partial S_1}{\partial z_2}      &  & \dots & \frac{\partial S_1}{\partial z_1}  \\
    \frac{\partial S_2}{\partial z_1}     & 
    \frac{\partial S_2}{\partial z_2}      &  & \dots & \frac{\partial S_2}{\partial z_C}\\
    \hdotsfor{5} \\
    \frac{\partial S_C}{\partial z_1}     & 
    \frac{\partial S_C}{\partial z_2}      &  & \dots & \frac{\partial S_C}{\partial z_C}\\
\end{bmatrix}\label{eq:matrix2}
\end{align}
In the equation \ref{eq:matrix2} we notice that we have only two "different" type of derivatives, $\frac{\partial S_i}{\partial z_i}, i \in \{ 1, ..., C\}$ and $\frac{\partial S_i}{\partial z_j}, i \ne j, i \in \{ 1, ..., C\}, j \in \{ 1, ..., C\}$.
\begin{align}
\frac{\partial S_i}{\partial z_i} &= 
\frac{\partial \frac{ e^{z_{i}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_i} \label{eq:partDer}\\
&= \frac{e^{z_{i}} \cdot \sum_{l=1}^{C}e^{z_{l}} - e^{z_{i}} e^{z_{i}}}
{\left(\sum_{l=1}^{C}e^{z_{l}}\right)^2}
\label{eq:divDer}\\
&= \frac{e^{z_{i}} \cdot \left(\sum_{l=1}^{C}e^{z_{l}} -  e^{z_{i}}\right)}
{\left(\sum_{l=1}^{C}e^{z_{l}}\right)^2}
\label{eq:simpleTrn}\\
&= \frac{e^{z_{i}}}{\sum_{l=1}^{C}e^{z_{l}}}\frac{ \sum_{l=1}^{C}e^{z_{l}} -  e^{z_{i}}}
{\sum_{l=1}^{C}e^{z_{l}}}
\label{eq:simpleTrn2}\\
&= S_i \cdot (1 - S_i) \label{eq:finalEq1}
\end{align}
The equation \ref{eq:partDer} uses $S_i$ definition. The equation \ref{eq:divDer} uses derivative of division rule. The equations \ref{eq:simpleTrn}, \ref{eq:simpleTrn2}, \ref{eq:finalEq1} are the simple transformations of the expressions and usage of definition $S_i$.
\begin{align}
\frac{\partial S_i}{\partial z_j} &= 
\frac{\partial \frac{ e^{z_{i}}}{\sum_{l=1}^{C}e^{z_{l}}}}{\partial z_j}
\label{eq:partDer2}\\
&=-\frac{e^{z_{i}}\cdot e^{z_{j}}}{\left(\sum_{l=1}^{C}e^{z_{l}}\right)^2} \label{eq:simpleTrn4}\\
&= -S_i \cdot S_j \label{eq:finalEq2}
\end{align}
The equation \ref{eq:partDer2} uses $S_i$ definition.
The equations \ref{eq:simpleTrn4} and \ref{eq:finalEq2} are simple arithmetic transformations of expressions expressions and usage of definition $S_i$.
Finally  we have:
\begin{align}
\frac{\partial \vect{a}}{\partial \vect{z}}  &=
\begin{bmatrix}
    \frac{\partial S_1}{\partial z_1}     & 
    \frac{\partial S_1}{\partial z_2}      &  & \dots & \frac{\partial S_1}{\partial z_1}  \\
    \frac{\partial S_2}{\partial z_1}     & 
    \frac{\partial S_2}{\partial z_2}      &  & \dots & \frac{\partial S_2}{\partial z_C}\\
    \hdotsfor{5} \\
    \frac{\partial S_C}{\partial z_1}     & 
    \frac{\partial S_C}{\partial z_2}      &  & \dots & \frac{\partial S_C}{\partial z_C}\\
\end{bmatrix} \nonumber \\
&=
\begin{bmatrix}
    S_1 \cdot (1- S_1)    & 
-S_1 \cdot S_2      &  & \dots & -S_1\cdot S_C \\
- S_2 \cdot S_1    & 
S_2 \cdot (1 - S_2)      &  & \dots & -S_2 \cdot S_C \\
    \hdotsfor{5} \\
        -S_C \cdot S_1    & 
-S_C \cdot S_1      &  & \dots & S_C \cdot (1 - S_C) \\
\end{bmatrix}\label{eq:derAZ}
\end{align}
The equation \ref{eq:derAZ} uses equations \ref{eq:finalEq1} and \ref{eq:finalEq2}.
Further we have:
\begin{align}
\frac{\partial \ell(\mat{W}, \vect{b}, \vect{x}, y)}{\partial \vect{z}} &=
\frac{\partial \ell (\mat{W}, \vect{b}, \vect{x}, y)}{\partial \vect{a}}
\cdot \frac{\partial \vect{a}}{\partial \vect{z}}
\label{eq:derRuleWZ}\\
&= \left[0, ..., -\frac{1}{a_y}, ..., 0\right] \cdot 
\begin{bmatrix}
    S_1 \cdot (1- S_1)    & 
-S_1 \cdot S_2      &  & \dots & -S_1\cdot S_C \\
- S_2 \cdot S_1    & 
S_2 \cdot (1 - S_2)      &  & \dots & -S_2 \cdot S_C \\
    \hdotsfor{5} \\
        -S_C \cdot S_1    & 
-S_C \cdot S_1      &  & \dots & S_C \cdot (1 - S_C) \\
\end{bmatrix} 
\label{eq:replaceDADZ}\\
&= \left[ \frac{S_y\cdot S_1 }{a_y},\frac{S_y \cdot S_2 }{a_y}, \dots, \frac{S_y \cdot (S_y - 1) }{a_y}, \dots, \frac{S_y \cdot S_C }{a_y}\right]
\label{eq:matMulAS}\\
&=  \left[ S_1 ,S_2, \dots, S_y - 1 \dots, S_C \right]
\label{eq:derWZ}
\end{align}
The equation \ref{eq:derRuleWZ} is just another way to write the same derivative. 
The equation \ref{eq:replaceDADZ} replaces derivatives using equations \ref{eq:derLog} and \ref{eq:derAZ}. The equation \ref{eq:matMulAS} is a matrix multiplication.  The equation \ref{eq:derWZ} is a a simplification of the equation \ref{eq:matMulAS} using the property that $a_i = S_i$ \footnote{I saw really late that $S_i = a_i$}. The Generalizing the formula $\frac { \partial \ell } { \partial w _ { i j } ^ { 2 } } = \frac { \partial \ell } { \partial z _ { i } ^ { 2 } } \cdot \frac { \partial z _ { i } ^ { 2 } } { \partial w _ { i j } ^ { 2 } } = \frac { \partial \ell } { \partial z _ { i } ^ { 2 } } \cdot x _ { j }$ we get:
\begin{align}
\frac{\partial \ell (\mat{W}, \vect{b}, \vect{x}, y)}{\partial \mat{W}} 
&=\transpose{\left( \vect{x} \frac{\partial \ell}{\partial \vect{z}}\right)} \nonumber \\
&=\begin{bmatrix}
    x_1 \cdot S_1   & 
x_2 \cdot S_1      &  & \dots  & x_D\cdot S_1 \\
 x_1 \cdot S_2    & 
x_2 \cdot S_2 &  & \dots & x_D \cdot S_2 \\
    \hdotsfor{5} \\
     x_1 \cdot (S_y -1)    & 
x_2 \cdot (S_y -1) &  & \dots & x_D \cdot (S_y -1) \\
    \hdotsfor{5} \\
        x_1 \cdot S_C    & 
x_2 \cdot S_C &     &  \dots& x_D \cdot  S_C \\
\end{bmatrix}\label{eq:matMulXLZ}
\end{align}
The equation \ref{eq:matMulXLZ} is a matrix multiplication of matrices. 
\begin{align}
\frac{\partial \ell(\mat{W}, \vect{b}, \vect{x}, y)}{\partial \vect{b}} &=
\frac{\partial \ell (\mat{W}, \vect{b}, \vect{x}, y)}{\partial \vect{z}}
\cdot \frac{\partial \vect{z}}{\partial \vect{b}} \label{eq:derRuleWb}
\end{align}
The equation \ref{eq:derRuleWb} is just another way to write derivative. 
If we notice that $\frac{\partial \vect{z}}{\partial \vect{b}}$ is an identity matrix ($b_i$ appears only in equality with the $z_i$), from the equation \ref{eq:derRuleWb} we get that $\frac{\partial \ell(\mat{W}, \vect{b}, \vect{x}, y)}{\partial \vect{b}}$ is the same as $\frac{\partial \ell(\mat{W}, \vect{b}, \vect{x}, y)}{\partial \vect{z}}$.
Using gradient descent for minibatch of $B$ training examples we get the following update equations:
$$\mat{W}_{t+1} = \mat{W}_t - \frac{1}{B}\sum_{i=1}^B\frac{\partial \ell(\vect{x}_i, y_i, \mat{W}_t, \vect{b}_t)}{\partial \mat{w}}$$
$$\vect{b}_{t+1} = \vect{b}_t - \frac{1}{B}\sum_{i=1}^B\frac{\partial \ell(\vect{x}_i, y_i, \mat{W}_t, \vect{b}_t)}{\partial \vect{b}}$$
We just need to adjust orientation of $\vect{b}$ and the derivative in the last update rule so the addition is possible. 
\item 
\begin{enumerate}
\item [1.]
I do not agree with the suggestion for several reasons. Let us notice that loss function of one training example $(\hat{\vect{y}_i}-\vect{y}_i)^2$ is just a sum of squares of errors (component-wise) of wrongly calculated outputs. The first reason is that in the binary coding approach not all errors are equal. If we encode 0 as 0000 and our neural network outputs 7, the error will be 3. On the other hand for the output 1 the error will be 1. This means, not all errors are equal, and they are related to the encoding. This could lead to unnecessarily longer training. Moreover, it could lead to stimulating errors with "lower" value in order to decrease the number of errors with bigger values.\\
The next problem is that our algorithm even when it is trained properly it could classify something incorrectly. For example in one hot encoding output with two ones is incorrect (we know there is some problem by seeing the output). On the other hand, in binary encoding we do not have "obvious" errors in encoding, because all outputs are valid (except the encodings for numbers bigger than 10).
The final problem is that it could stimulate some neurons to have lower values because specific digit in encoding is not frequent enough. For example the first digit in binary encoding appears only in 2 numbers (9 and 10), but the network may believe it is not so important and it could decrease the value. 
\item [2.]

\begin{figure}
\centering

\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{power neuron}=[neuron, fill=yellow!50];
    \tikzstyle{binary neuron}=[neuron, fill=cyan!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,2,3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input \#\y](\y) (I-\name) at (0,-\y) {$x_\y$};
    \edef\mya{782}
    \foreach \name / \y in {5,6,7}
    	\pgfmathparse{int(\mya+1)}
    	\xdef\mya{\pgfmathresult}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input \#\y](\y) (I-\name) at (0,-\y) {$x_{\mya}$};
	\foreach \name / \y in {4}	
		\node[] at (0,-\y) {$\vdots$};
	% Draw the input layer nodes
 
    % Draw the hidden layer nodes
    \edef\ya{0}
    \foreach \name / \y in {1,...,15}
    	\pgfmathparse{\ya+0.6}
    	\xdef\ya{\pgfmathresult}
        \path[yshift=0.5cm]
            node[hidden neuron] (H-\name) at (\layersep,-\ya cm) {};
	

    % Draw the output layer node
	\edef\ya{0}
    \foreach \name / \y in {1,...,10}
    	\pgfmathparse{\ya+0.6}
    	\xdef\ya{\pgfmathresult}
    	\path[yshift=-0.7cm]
   	 		node[output neuron] (O-\name) at (\layersepH, -\ya cm) {$y_{\y}$};
	
	 % Draw the pow layer node
	\edef\ya{0}
    \foreach \name / \y in {1,...,10}
    	\pgfmathparse{\ya+0.6}
    	\xdef\ya{\pgfmathresult}
    	\path[yshift=-0.7cm]
   	 		node[power neuron] (P-\name) at (\layersepP, -\ya cm) {$p_{\y}$};	
	
	% Draw the binary layer node
	\edef\ya{2}
    \foreach \name / \y in {1,...,4}
    	\pgfmathparse{\ya+0.6}
    	\xdef\ya{\pgfmathresult}
    	\path[yshift=-0.7cm]
   	 		node[binary neuron, pin={[pin edge={->}]right:Output}] (B-\name) at (\layersepB, -\ya cm) {$d_{\y}$};	
	
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,2, 3, 5, 6, 7}
        \foreach \dest in {1,...,15}
            \path (I-\source) edge (H-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,15}
	    \foreach \dest in {1,...,10}
    	    \path (H-\source) edge (O-\dest);
    
 	% Connect every node in the output layer with the hidden layer
    \foreach \source in {1,...,10}
    	\path (O-\source) edge (P-\source); 
	\foreach \source in {1,...,10}
		\foreach \dest in {1,...,4}
    		\path (P-\source) edge (B-\dest);        
    
    % Annotate the layers
    \node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer};
    \node[annot,left of=hl] {Input layer};
    \node[annot,above of=O-1, node distance=2cm] {One-hot encoded layer};
    \node[annot,above of=P-1, node distance=1cm] {Power layer};
    \node[annot,above of=B-1] {Binary layer};
\end{tikzpicture}
\caption{Neural network that transforms one hot-encoded output to binary output}
\label{fig:NNPow}
\end{figure}
On the figure \ref{fig:NNPow} is shown the neural network that fulfills requested in the task. $p_i(x) = bs(x - p)$ is the activation function, which for $x> p$ returns 1, and for $x\leq p$ returns 0. We choose $p$ that suits the best, $p=0.5$ is a reasonable choice. Further, weight $(p_i, d_j)$ is 1 only in those cases where in binary representation of number $i$ on $j$-th place is 1. So for example for $p_7$ the weights towards $d_1, d_2, d_3, d_4$ will be $1, 1, 1, 0$, for $p_9$ the weights will be $1, 0, 0, 1$ etc. We can say we added two additional layers, because power layer has just activation functions binary step, and the input weights are all one. The output layer is just linear activation function, whose weights are previously stated. Why the network gives the correct output is obvious, because only in cases where $y_i>p$ $p_i$-th node will be 1, and further binary layer will generate corresponding representation of the $i$-th number. 
\item [3.]
The same problem as for the first subproblem of this problem, because we minimize the objective functions, thus the different errors will have the different influence on nodes. 
\end{enumerate}

\begin{enumerate}
\item[1.]
sdfsdfdf
\end{enumerate}
\end{enumerate}

\end{document}