\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Djordje Zivanovic}
\title{Second Homework from Machine Learning}
\begin{document}

\centerline{\large \bf Machine learning : Sheet 2}
\centerline{\large Author : Djordje Zivanovic}
\bigskip
%Chapter 1 in Ockendon \& Tayler: Applied PDE's 
%{\it Maths Institute Lecture Notes}. 
%{\bf Exercises 1, 2, 4, 5}.
\newcommand{\transpose}[1]{#1 ^ \text{T}}
\newcommand{\expectation}{\underset{\mathcal{D}}{\mathbb{E}}}

\begin{enumerate}
\item
On the lectures we got the following equation:
$$\text{NLL}(\textbf{y}\,\vert\,\textbf{X}, \textbf{w}, \sigma)= \frac{1}{2\sigma^2}(\textbf{Xw} - \textbf{y})^\text{T}(\textbf{Xw} - \textbf{y}) + \frac{N}{2}\log(2\pi\sigma^2)$$
By setting derivative over $\sigma$ to 0, we get the following:
\begin{align*}
\frac{\partial \text{NLL}(\textbf{y}\,\vert\,\textbf{X}, \textbf{w}, \sigma)}{\partial \sigma} &= \frac{-2\sigma^{-3}}{2} (\textbf{Xw} - \textbf{y})^\text{T}(\textbf{Xw} - \textbf{y}) + 
\frac{N}{2}\frac{2\pi 2\sigma}{2\pi\sigma^2}\\
&= -\sigma^{-3} (\textbf{Xw} - \textbf{y})^\text{T}(\textbf{Xw} - \textbf{y}) + \frac{N}{\sigma} = 0
\end{align*}
Multiplying the last expression with $\sigma^3$, because variance cannot be 0, and dividing by $N$ we get $$\sigma^2 =\frac{(\textbf{Xw} - \textbf{y})^\text{T}(\textbf{Xw} - \textbf{y})}{N} $$
Since optimal $\textbf{w}$ is calculated by setting gradient to zero, we insert $\textbf{w}_{\textbf{ML}}$ to the last equation and get the necessary $\sigma$.
\item By using vector multiplication rules we can simplify the expression:
\begin{align*}
\mathcal{L}_{ridge}(\textbf{w},b) 
&= \transpose{(\textbf{Xw}+b\textbf{1}-\textbf{y})}(\textbf{Xw}+b\textbf{1}-\textbf{y})+\lambda \transpose{\textbf{w}}\textbf{w}\\
&= \transpose{\textbf{w}}\transpose{\textbf{X}}\textbf{Xw} + \transpose{\textbf{w}}\transpose{\textbf{X}}b\textbf{1}-\transpose{\textbf{w}}\transpose{\textbf{X}}\textbf{y}+b\transpose{\textbf{1}}\textbf{X}\textbf{w} + b^2N -b\transpose{\textbf{1}}\textbf{y} - \transpose{\textbf{y}}\textbf{Xw}-b\transpose{y}\textbf{1} + \transpose{\textbf{y}} \textbf{y} + \lambda \transpose{\textbf{w}}\textbf{w}\\
\end{align*}
Deriving $\mathcal{L}_{ridge}(\textbf{w},b) $ per $b$ and equaling to zero we get:
\begin{align*}
\frac{\partial\mathcal{L}_{ridge}(\textbf{w},b)}{\partial b} &= 
0 + \transpose{\textbf{w}}\transpose{\textbf{X}}\textbf{1} - 0 + \transpose{\textbf{1}}\textbf{X}\textbf{w} + 2bN - \transpose{\textbf{1}}\textbf{y} - 0  - \transpose{y}\textbf{1} + 0 + 0\\
&= \transpose{\textbf{w}}\transpose{\textbf{X}}\textbf{1}  + \transpose{\textbf{1}}\textbf{X}\textbf{w}+ 2bN - 2\transpose{\textbf{y}}\textbf{1} = 0
\end{align*}
Using the property that product of $\transpose{\textbf{A}}B=\transpose{B}A$ when $\textbf{A,B}$ are column vectors we got the simplification of the last equation.
Further, $\transpose{\textbf{X}}$ when multiplied by $\textbf{1}$ from column matrix whose cells are the sum of the same features. From the beginning condition that sum for every possible set of features is 0, the first two addends will give 0 as a result (associativity of matrix multiplication). Thus, we get $b = \transpose{\textbf{y}}$ which is equivalent to $\hat{b}=\frac{1}{N}\sum_{i=1}^Ny_i$.
\\
Deriving $\mathcal{L}_{ridge}(\textbf{w},b) $ per $\textbf{w}$ and equaling to zero we get:
\begin{align*}
\frac{\partial\mathcal{L}_{ridge}(\textbf{w},b)}{\partial \textbf{w}} &= 
2\transpose{\textbf{X}}\textbf{X}\textbf{w} + 0 -  \transpose{\textbf{X}}\textbf{y} + 0 + 0 - 0 - \transpose{\textbf{X}}\textbf{y }- 0 + 0 +2\lambda\textbf{w}\\
&= 2\transpose{\textbf{X}}\textbf{X}\textbf{w} -2 \transpose{\textbf{X}}\textbf{y}  + 2\lambda\textbf{w} = 0
\end{align*}
In the previous derivation we simplified some expression (set them to zero) using the same observation $\transpose{\textbf{X}}\textbf{1}=0$. Using distributivity of matrix multiplication $$ (\transpose{\textbf{X}}\textbf{X} + \lambda \textbf{I}_D)\textbf{w} = \transpose{\textbf{X}}\textbf{y} $$ 
Multiplying the both sides by $(\transpose{\textbf{X}}\textbf{X} + \lambda \textbf{I}_D)$ we get the $\hat{\textbf{w}}=(\transpose{\textbf{X}}\textbf{X} + \lambda \textbf{I}_D)^{-1}\transpose{\textbf{X}}\textbf{y}$.
\\
Consequence of centering $\textbf{y}$ gives us that bias is unnecessary in this case (we would need neither to calculate nor to use it).
\item 
\begin{enumerate}
\item[1.]
Using properties of expectation:
\begin{align}
\mathbb{E}(\hat{\textbf{w}}_\text{LS}(\mathcal{D}) \vert \mathcal{D}) 
&= \mathbb{E}((\transpose{\textbf{X}}X)^{-1}\transpose{\textbf{X}}\textbf{y}\vert \mathcal{D})\nonumber \\
&= (\transpose{\textbf{X}}\textbf{X})^{-1}\transpose{\textbf{X}}\mathbb{E}(\textbf{y}\vert \mathcal{D}) \label{eq:const} \\
&= (\transpose{\textbf{X}}\textbf{X})^{-1}\transpose{\textbf{X}}\textbf{X}\textbf{w}^* \label{eq:task}\\
&= I\textbf{w}^* = \textbf{w}^*  \nonumber
\end{align}
The equation \ref{eq:const}is correct by using property of expected value that it is possible to pull out constant from the parenthesis. The equation \ref{eq:task} is correct because it is stated that $\mathbb{E}(\textbf{y}\vert \mathcal{D})=\textbf{X}\textbf{w}^* $ in the text of the task. 
\item[2.]
\begin{align}
\expectation \left[ \vert\vert \hat{\textbf{w}}(\mathcal{D})-\textbf{w}^*\vert \vert ^2\right]
 &= \underset{\mathcal{D}}{\mathbb{E}}\left[ 
\transpose{(\hat{\textbf{w}}(\mathcal{D})-\textbf{w}^*)}(\hat{\textbf{w}}(\mathcal{D})-\textbf{w}^*)\right] \label{eq:product}\\
&= \expectation\left[ 
\transpose{\hat{\textbf{w}}(\mathcal{D})} \hat{\textbf{w}}(\mathcal{D}) - 
\transpose{(\textbf{w}^*)} \hat{\textbf{w}}(\mathcal{D}) - 
\transpose{\hat{\textbf{w}}(\mathcal{D})} \textbf{w}^*
+ \transpose{(\textbf{w}^*)} \textbf{w}^*
\right] \label{eq:dis}\\
&= \expectation \left[ 
\transpose{(\hat{\textbf{w}}(\mathcal{D}))} \hat{\textbf{w}}(\mathcal{D})\right] - 
\transpose{(\textbf{w}^*)} \expectation \left[ \hat{\textbf{w}}(\mathcal{D})\right] - 
\expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))} \right] \textbf{w}^*
+ \transpose{(\textbf{w}^*)} \textbf{w}^* \label{eq:prop}\\
&=
\expectation \left[ 
\transpose{(\hat{\textbf{w}}(\mathcal{D}))} \hat{\textbf{w}}(\mathcal{D})\right] - 
2\transpose{(\textbf{w}^*)} \expectation \left[ \hat{\textbf{w}}(\mathcal{D})\right]
+ \transpose{(\textbf{w}^*)} \textbf{w}^*\label{eq:last}
\end{align}

The equation \ref{eq:product} is the other way to calculate Eucleadean norm using scalar product. The equation \ref{eq:dis} is simple distributivity law. The equation \ref{eq:prop} uses properties of mathematical expectation. The last equation \ref{eq:last} uses only a property that a scalar product of a column and a row vector is equal to their transposed product. 
\begin{align}
\vert \vert \expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]-\textbf{w}^*  \vert \vert ^2 &= 
\transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]-\textbf{w}^*)} (\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]-\textbf{w}^*)
\label{eq:product2}\\
&= 
\transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right])}
\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]
- \transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right])}\textbf{w}^*
-\transpose{(\textbf{w}^*)} 
\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]
+\transpose{(\textbf{w}^*)} \textbf{w}^* 
\label{eq:dis2}\\
&=
\transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right])}
\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]
- 2\transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right])}\textbf{w}^*
+\transpose{(\textbf{w}^*)} \textbf{w}^*
\label{eq:last2}
\end{align}
The equation \ref{eq:product2} is the other way to calculate Eucleadean norm using scalar product. The equation \ref{eq:dis2} is simple distributivity law. The last equation \ref{eq:last2} uses only a property that a scalar product of a column and a row vector is equal to their transposed product. 
\begin{align}
\expectation\left[ \vert \vert
 \hat{\textbf{w}}(\mathcal{D}) - 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]
 \vert \vert^2\right]
 &= \expectation \left[ \transpose{\left(\hat{\textbf{w}}(\mathcal{D}) - 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] \right)}
 \left(\hat{\textbf{w}}(\mathcal{D}) - 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)
  \right] \label{eq:product3} \\
 &= 
 \expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))}
 \hat{\textbf{w}}(\mathcal{D})
 -\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)}
 \hat{\textbf{w}}(\mathcal{D})
 -\transpose{(\hat{\textbf{w}}(\mathcal{D}))}
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] 
 +\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)} 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] 
 \right] \label{eq:dis3}\\
 &=  \expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))}
 \hat{\textbf{w}}(\mathcal{D})
 \right]
 -\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)}
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]-
 \nonumber \\
 &-\expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))} \right]
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] 
 +\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)} 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] 
\label{eq:prop3} \\ 
 &=\expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))}
 \hat{\textbf{w}}(\mathcal{D})
 \right]
 -\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)} 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]
 \label{eq:last3}
\end{align}

The equation \ref{eq:product3} is the other way to calculate Eucleadean norm using scalar product. The equation \ref{eq:dis3} is simple distributivity law.  The equation \ref{eq:prop3} uses properties of mathematical expectation. The last equation \ref{eq:last3} uses only a property that a scalar product of a column and a row vector is equal to their transposed product. 
\\
By adding equations \ref{eq:last2} and \ref{eq:last3} we get the expression from \ref{eq:last}.
\end{enumerate}

\end{enumerate}

\end{document}