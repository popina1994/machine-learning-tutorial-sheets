\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Djordje Zivanovic}
\title{Second Homework from Machine Learning}
\begin{document}

\centerline{\large \bf Machine learning : Sheet 2}
\centerline{\large Author : Djordje Zivanovic}
\bigskip
%Chapter 1 in Ockendon \& Tayler: Applied PDE's 
%{\it Maths Institute Lecture Notes}. 
%{\bf Exercises 1, 2, 4, 5}.
\newcommand{\transpose}[1]{#1 ^ \text{T}}
\newcommand{\expectation}{\underset{\mathcal{D}}{\mathbb{E}}}

\begin{enumerate}
\item
On the lectures we got the following equation:
$$\text{NLL}(\textbf{y}\,\vert\,\textbf{X}, \textbf{w}, \sigma)= \frac{1}{2\sigma^2}(\textbf{Xw} - \textbf{y})^\text{T}(\textbf{Xw} - \textbf{y}) + \frac{N}{2}\log(2\pi\sigma^2)$$
By setting derivative over $\sigma$ to 0, we get the following:
\begin{align*}
\frac{\partial \text{NLL}(\textbf{y}\,\vert\,\textbf{X}, \textbf{w}, \sigma)}{\partial \sigma} &= \frac{-2\sigma^{-3}}{2} (\textbf{Xw} - \textbf{y})^\text{T}(\textbf{Xw} - \textbf{y}) + 
\frac{N}{2}\frac{2\pi 2\sigma}{2\pi\sigma^2}\\
&= -\sigma^{-3} (\textbf{Xw} - \textbf{y})^\text{T}(\textbf{Xw} - \textbf{y}) + \frac{N}{\sigma} = 0
\end{align*}
Multiplying the last expression with $\sigma^3$, because variance cannot be 0, and dividing by $N$ we get $$\sigma^2 =\frac{(\textbf{Xw} - \textbf{y})^\text{T}(\textbf{Xw} - \textbf{y})}{N} $$
Since optimal $\textbf{w}$ is calculated by setting gradient to zero, we insert $\textbf{w}_{\textbf{ML}}$ to the last equation and get the necessary $\sigma$.
\item By using vector multiplication rules we can simplify the expression:
\begin{align*}
\mathcal{L}_{ridge}(\textbf{w},b) 
&= \transpose{(\textbf{Xw}+b\textbf{1}-\textbf{y})}(\textbf{Xw}+b\textbf{1}-\textbf{y})+\lambda \transpose{\textbf{w}}\textbf{w}\\
&= \transpose{\textbf{w}}\transpose{\textbf{X}}\textbf{Xw} + \transpose{\textbf{w}}\transpose{\textbf{X}}b\textbf{1}-\transpose{\textbf{w}}\transpose{\textbf{X}}\textbf{y}+b\transpose{\textbf{1}}\textbf{X}\textbf{w} + b^2N -b\transpose{\textbf{1}}\textbf{y} - \transpose{\textbf{y}}\textbf{Xw}-b\transpose{y}\textbf{1} + \transpose{\textbf{y}} \textbf{y} + \lambda \transpose{\textbf{w}}\textbf{w}\\
\end{align*}
Deriving $\mathcal{L}_{ridge}(\textbf{w},b) $ per $b$ and equaling to zero we get:
\begin{align*}
\frac{\partial\mathcal{L}_{ridge}(\textbf{w},b)}{\partial b} &= 
0 + \transpose{\textbf{w}}\transpose{\textbf{X}}\textbf{1} - 0 + \transpose{\textbf{1}}\textbf{X}\textbf{w} + 2bN - \transpose{\textbf{1}}\textbf{y} - 0  - \transpose{y}\textbf{1} + 0 + 0\\
&= \transpose{\textbf{w}}\transpose{\textbf{X}}\textbf{1}  + \transpose{\textbf{1}}\textbf{X}\textbf{w}+ 2bN - 2\transpose{\textbf{y}}\textbf{1} = 0
\end{align*}
Using the property that product of $\transpose{\textbf{A}}B=\transpose{B}A$ when $\textbf{A,B}$ are column vectors we got the simplification of the last equation.
Further, $\transpose{\textbf{X}}$ when multiplied by $\textbf{1}$ from column matrix whose cells are the sum of the same features. From the beginning condition that sum for every possible set of features is 0, the first two addends will give 0 as a result (associativity of matrix multiplication). Thus, we get $b = \transpose{\textbf{y}}$ which is equivalent to $\hat{b}=\frac{1}{N}\sum_{i=1}^Ny_i$.
\\
Deriving $\mathcal{L}_{ridge}(\textbf{w},b) $ per $\textbf{w}$ and equaling to zero we get:
\begin{align*}
\frac{\partial\mathcal{L}_{ridge}(\textbf{w},b)}{\partial \textbf{w}} &= 
2\transpose{\textbf{X}}\textbf{X}\textbf{w} + 0 -  \transpose{\textbf{X}}\textbf{y} + 0 + 0 - 0 - \transpose{\textbf{X}}\textbf{y }- 0 + 0 +2\lambda\textbf{w}\\
&= 2\transpose{\textbf{X}}\textbf{X}\textbf{w} -2 \transpose{\textbf{X}}\textbf{y}  + 2\lambda\textbf{w} = 0
\end{align*}
In the previous derivation we simplified some expression (set them to zero) using the same observation $\transpose{\textbf{X}}\textbf{1}=0$. Using distributivity of matrix multiplication $$ (\transpose{\textbf{X}}\textbf{X} + \lambda \textbf{I}_D)\textbf{w} = \transpose{\textbf{X}}\textbf{y} $$ 
Multiplying the both sides by $(\transpose{\textbf{X}}\textbf{X} + \lambda \textbf{I}_D)$ we get the $\hat{\textbf{w}}=(\transpose{\textbf{X}}\textbf{X} + \lambda \textbf{I}_D)^{-1}\transpose{\textbf{X}}\textbf{y}$.
\\
Consequence of centering $\textbf{y}$ gives us that bias is unnecessary in this case (we would need neither to calculate nor to use it).
\item 
\begin{enumerate}
\item[1.]
Using properties of expectation:
\begin{align}
\mathbb{E}(\hat{\textbf{w}}_\text{LS}(\mathcal{D}) \vert \mathcal{D}) 
&= \mathbb{E}((\transpose{\textbf{X}}X)^{-1}\transpose{\textbf{X}}\textbf{y}\vert \mathcal{D})\nonumber \\
&= (\transpose{\textbf{X}}\textbf{X})^{-1}\transpose{\textbf{X}}\mathbb{E}(\textbf{y}\vert \mathcal{D}) \label{eq:const} \\
&= (\transpose{\textbf{X}}\textbf{X})^{-1}\transpose{\textbf{X}}\textbf{X}\textbf{w}^* \label{eq:task}\\
&= I\textbf{w}^* = \textbf{w}^*  \nonumber
\end{align}
The equation \ref{eq:const}is correct by using property of expected value that it is possible to pull out constant from the parenthesis. The equation \ref{eq:task} is correct because it is stated that $\mathbb{E}(\textbf{y}\vert \mathcal{D})=\textbf{X}\textbf{w}^* $ in the text of the task. 
\item[2.]
\begin{align}
\expectation \left[ \vert\vert \hat{\textbf{w}}(\mathcal{D})-\textbf{w}^*\vert \vert ^2\right]
 &= \underset{\mathcal{D}}{\mathbb{E}}\left[ 
\transpose{(\hat{\textbf{w}}(\mathcal{D})-\textbf{w}^*)}(\hat{\textbf{w}}(\mathcal{D})-\textbf{w}^*)\right] \label{eq:product}\\
&= \expectation\left[ 
\transpose{\hat{\textbf{w}}(\mathcal{D})} \hat{\textbf{w}}(\mathcal{D}) - 
\transpose{(\textbf{w}^*)} \hat{\textbf{w}}(\mathcal{D}) - 
\transpose{\hat{\textbf{w}}(\mathcal{D})} \textbf{w}^*
+ \transpose{(\textbf{w}^*)} \textbf{w}^*
\right] \label{eq:dis}\\
&= \expectation \left[ 
\transpose{(\hat{\textbf{w}}(\mathcal{D}))} \hat{\textbf{w}}(\mathcal{D})\right] - 
\transpose{(\textbf{w}^*)} \expectation \left[ \hat{\textbf{w}}(\mathcal{D})\right] - 
\expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))} \right] \textbf{w}^*
+ \transpose{(\textbf{w}^*)} \textbf{w}^* \label{eq:prop}\\
&=
\expectation \left[ 
\transpose{(\hat{\textbf{w}}(\mathcal{D}))} \hat{\textbf{w}}(\mathcal{D})\right] - 
2\transpose{(\textbf{w}^*)} \expectation \left[ \hat{\textbf{w}}(\mathcal{D})\right]
+ \transpose{(\textbf{w}^*)} \textbf{w}^*\label{eq:last}
\end{align}

The equation \ref{eq:product} is the other way to calculate Eucleadean norm using scalar product. The equation \ref{eq:dis} is simple distributivity law. The equation \ref{eq:prop} uses properties of mathematical expectation. The last equation \ref{eq:last} uses only a property that a scalar product of a column and a row vector is equal to their transposed product. 
\begin{align}
\vert \vert \expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]-\textbf{w}^*  \vert \vert ^2 &= 
\transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]-\textbf{w}^*)} (\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]-\textbf{w}^*)
\label{eq:product2}\\
&= 
\transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right])}
\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]
- \transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right])}\textbf{w}^*
-\transpose{(\textbf{w}^*)} 
\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]
+\transpose{(\textbf{w}^*)} \textbf{w}^* 
\label{eq:dis2}\\
&=
\transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right])}
\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right]
- 2\transpose{(\expectation \left[  \hat{\textbf{w}}(\mathcal{D})\right])}\textbf{w}^*
+\transpose{(\textbf{w}^*)} \textbf{w}^*
\label{eq:last2}
\end{align}
The equation \ref{eq:product2} is the other way to calculate Eucleadean norm using scalar product. The equation \ref{eq:dis2} is simple distributivity law. The last equation \ref{eq:last2} uses only a property that a scalar product of a column and a row vector is equal to their transposed product. 
\begin{align}
\expectation\left[ \vert \vert
 \hat{\textbf{w}}(\mathcal{D}) - 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]
 \vert \vert^2\right]
 &= \expectation \left[ \transpose{\left(\hat{\textbf{w}}(\mathcal{D}) - 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] \right)}
 \left(\hat{\textbf{w}}(\mathcal{D}) - 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)
  \right] \label{eq:product3} \\
 &= 
 \expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))}
 \hat{\textbf{w}}(\mathcal{D})
 -\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)}
 \hat{\textbf{w}}(\mathcal{D})
 -\transpose{(\hat{\textbf{w}}(\mathcal{D}))}
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] 
 +\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)} 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] 
 \right] \label{eq:dis3}\\
 &=  \expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))}
 \hat{\textbf{w}}(\mathcal{D})
 \right]
 -\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)}
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]-
 \nonumber \\
 &-\expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))} \right]
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] 
 +\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)} 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right] 
\label{eq:prop3} \\ 
 &=\expectation \left[ \transpose{(\hat{\textbf{w}}(\mathcal{D}))}
 \hat{\textbf{w}}(\mathcal{D})
 \right]
 -\transpose{\left(\expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]\right)} 
 \expectation \left[ \hat{\textbf{w}}(\mathcal{D}) \right]
 \label{eq:last3}
\end{align}

The equation \ref{eq:product3} is the other way to calculate Eucleadean norm using scalar product. The equation \ref{eq:dis3} is simple distributivity law.  The equation \ref{eq:prop3} uses properties of mathematical expectation. The last equation \ref{eq:last3} uses only a property that a scalar product of a column and a row vector is equal to their transposed product. 
\\
By adding equations \ref{eq:last2} and \ref{eq:last3} we get the expression from the equation \ref{eq:last}.
\end{enumerate}
\item
\begin{enumerate}
\item[1.]
We can notice that $$p(x, y \vert \mathbf{\theta})=p (y\vert, x, \mathbf{\theta})p(x|\mathbf{\theta})$$
Inserting corresponding probabilities we get the table as on the figure \ref{fig:Prob}.

\begin{figure}[htb!]
\begin{center}
\begin{tabular}{c|cc}
 & $y=0$ & $y=1$ \\ 
\hline 
$x=0$ & $\theta_2(1-\theta_1)$ & $(1-\theta_2)(1-\theta_1)$ \\ 
$x=1$ & $(1-\theta_2)\theta_1$ & $\theta_2\theta1$ \\ 
\end{tabular} 
\end{center}
\caption{Probabilities}
\label{fig:Prob}
\end{figure}

\item[2.]
If we denote with $\alpha_1, \alpha_2, \alpha_3, \alpha_4$, respectively, the number of events where tuple $(x, y)$ is $(0, 0), (0, 1), (1, 0), (1, 1)$, we have the following:
\begin{align*}
MLE(p(x, y|\theta_1, \theta_2)) &= \theta_2^{\alpha_1}(1-\theta_1)^{\alpha_1}(1-\theta_2)^{\alpha_2}(1-\theta_1)^{\alpha_2}(1-\theta_2)^{\alpha_3}\theta_1^{\alpha_3}\theta_2^{\alpha_4}\theta_1^{\alpha_4} \nonumber \\
&= \theta_1^{\alpha_3 + \alpha_4}\theta_2^{\alpha_1+\alpha4}
(1-\theta_1)^{\alpha_1 + \alpha_2}(1-\theta_2)^{\alpha_2 + \alpha_3}
\end{align*}
By calculating derivatives $MLE(p(x, y|\theta_1, \theta_2$ per $\theta_1, \theta_2)$ and setting them to 0 we get:
\begin{align*}
\frac{\partial MLE(p(x, y|\theta_1, \theta_2))}{\partial \theta_1}&= (\alpha_3+\alpha_4)\theta_1^{\alpha_3+\alpha_4 -1}\theta_2^{\alpha_1+\alpha4}
(1-\theta_1)^{\alpha_1 + \alpha_2}(1-\theta_2)^{\alpha_2 + \alpha_3}\\
&- (\alpha_1+\alpha_2)\theta_1^{\alpha_3+\alpha_4}\theta_2^{\alpha_1+\alpha4}
(1-\theta_1)^{\alpha_1 + \alpha_2-1}(1-\theta_2)^{\alpha_2 + \alpha_3}=0
\end{align*}
By dividing both side of equation by $\theta_1^{\alpha_3+\alpha_4-1}\theta_2^{\alpha_1+\alpha4}
(1-\theta_1)^{\alpha_1 + \alpha_2-1}(1-\theta_2)^{\alpha_2 + \alpha_3}$ we get:
\begin{align*}
(\alpha_3+\alpha_4)(1-\theta_1)- (\alpha_1 + \alpha_2)\theta_1 = 0
\end{align*}
Thus $\theta_1 = \frac{\alpha_3+\alpha_4}{\alpha_1+\alpha_2 + \alpha_3+\alpha_4}$. Similarly $\theta_2 = \frac{\alpha_1+\alpha_4}{\alpha_1+\alpha_2 + \alpha_3+\alpha_4}$
Replacing the values from our dataset we get $\theta_1 = \frac{4}{7}$ and $\theta_2=\frac{4}{7}$. The $p(\mathcal{D}|\hat{\theta},M_2)$ is given on figure
\ref{fig:prediction}. If we calculated MLE for $p(x|\theta)$ and $p(y|\theta)$ we would get the same probabilities as we can see that $\theta_1$ is not dependent on $\theta_2$ and vice versa in $p(x, y|\theta)$.
\begin{figure}[htb!]
\begin{center}
\begin{tabular}{c|cc}
 & $y=0$ & $y=1$ \\ 
\hline 
$x=0$ & $\frac{12}{49}$ & $\frac{9}{49}$ \\ 
$x=1$ & $\frac{12}{49}$ & $\frac{16}{49}$ \\ 
\end{tabular} 
\end{center}
\caption{Probabilities}
\label{fig:prediction}
\end{figure} 
\item[3.]
Let us denote number of occurrences  of each tuple $x, y$, $(0, 0), (0, 1), (1, 0), (1, 1)$ respectively, with $\alpha_1, \alpha_2, \alpha_3, \alpha_4$. Then we would like to maximize:
$$MLE=\theta_{0,0}^{\alpha_1}\theta_{0, 1}^{\alpha_2}\theta_{1, 0}^{\alpha_3}\theta_{1, 1}^{\alpha_4} =\theta_{0,0}^{\alpha_1}\theta_{0, 1}^{\alpha_2}\theta_{1, 0}^{\alpha_3}(1-\theta_{0, 0}-\theta_{0, 1}-\theta_{1, 0})^{\alpha_4}$$ 
Thus, let us change $A = \theta_{0, 0}, B=\theta_{0, 1}, C=\theta_{1, 0}$. Further, due to maximization we need to have derivative equal to zero:
$$\frac{\partial MLE}{\partial A} =\alpha_1 A^{\alpha_1-1}B^{\alpha_2}C^{\alpha_3}(1-A-B-C)^{\alpha_4} -\alpha_4 A^{\alpha_1}B^{\alpha_2}C^{\alpha_3}(1-A-B-C)^{\alpha_4 -1}=0 $$


	Dividing both sides of equation by $A^{\alpha_1-1}B^{\alpha_2}C^{\alpha_3}(1-A-B-C)^{\alpha_4 -1}$ we get:
$$\alpha_1(1-A-B-C)-\alpha_4A=0$$
$$(\alpha_1 + \alpha_4 )A + \alpha_1B +\alpha_1C = \alpha_1$$
Similarly:
$$(\alpha_2 + \alpha_4 )B + \alpha_2A +\alpha_2C = \alpha_2$$
$$(\alpha_3 + \alpha_4 )C + \alpha_3A +\alpha_3B = \alpha_3$$
The determinant is:
$\begin{vmatrix}
\alpha_1 + \alpha_4 & \alpha_1 & \alpha_1 \\ 
\alpha_2 & \alpha_2 + \alpha_4 & \alpha_2 \\ 
\alpha_3 & \alpha_3 &  \alpha_3 + \alpha_4\\ 
\end{vmatrix}$
which is positive always (because the result is the sum of positive monomials).
This means the system has a unique solution.
If we try the solution: $$(A, B, C) = \left(\frac{\alpha_1}{\alpha_1 + \alpha_2+\alpha_3+\alpha_4},
 \frac{\alpha_2}{\alpha_1 + \alpha_2+\alpha_3+\alpha_4}
 \frac{\alpha_3}{\alpha_1 + \alpha_2+\alpha_3+\alpha_4}\right)$$ we can see this tuple satisfies solutions. 
 \item [4.]
 For the data set give in subtask 2. the 2-parameter model will be chosen (if we do all calculations manually), because the 4-parameter model overfits the data.
 IN generals case the calculations would have to be done, but problem of overfitting would limit the 4-parameter model ability to learn more widely.  
\end{enumerate}
\end{enumerate}

\end{document}